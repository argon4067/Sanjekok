import time
import requests
from io import BytesIO
from PIL import Image
from pathlib import Path
from news.models import News
from django.conf import settings
import hashlib

# ì €ì¥ í´ë”
NEWS_IMG_DIR = Path(settings.BASE_DIR) / "static/img/news"
NEWS_IMG_DIR.mkdir(parents=True, exist_ok=True)  # í´ë” ì—†ìœ¼ë©´ ìƒì„±

def download_news_image(url, save_name, max_size_kb=100):
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        img = Image.open(BytesIO(response.content))
        img_format = img.format if img.format else "PNG"

        buffer = BytesIO()
        quality = 95

        if img_format.upper() in ["JPEG", "JPG"]:
            img.save(buffer, format="JPEG", quality=quality)
        else:
            img.save(buffer, format="PNG", optimize=True)

        while buffer.getbuffer().nbytes > max_size_kb * 1024 and quality > 10:
            buffer = BytesIO()
            if img_format.upper() in ["JPEG", "JPG"]:
                quality -= 5
                img.save(buffer, format="JPEG", quality=quality)
            else:
                width, height = img.size
                img = img.resize((int(width * 0.9), int(height * 0.9)))
                img.save(buffer, format="PNG", optimize=True)

        # ì‹¤ì œ ì €ì¥ ê²½ë¡œ
        save_path = NEWS_IMG_DIR / save_name
        with open(save_path, "wb") as f:
            f.write(buffer.getvalue())

        return f"img/news/{save_name}"  # DBì— ì €ì¥í•  ìƒëŒ€ê²½ë¡œ

    except Exception as e:
        print("ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨:", e)
        return None


def crawl_news_images(max_size_kb=100):
    print("\n===== ğŸŸ¢ ë‰´ìŠ¤ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘ =====")

    qs = News.objects.filter(
        n_image_url__startswith="http"
    ).order_by("-n_created_at")

    processed_urls = {}  # URLë³„ ì²˜ë¦¬ëœ ë¡œì»¬ ê²½ë¡œë¥¼ ìºì‹œí•˜ëŠ” ë”•ì…”ë„ˆë¦¬

    for idx, news in enumerate(qs, start=1):
        external_url = news.n_image_url
        print(f"ğŸ–¼ï¸  {idx}/{len(qs)} ì²˜ë¦¬ ì¤‘: {external_url[:80]}")

        # 1. ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if external_url in processed_urls:
            news.n_image_url = processed_urls[external_url]
            news.save(update_fields=["n_image_url"])
            print(f"  -> ìºì‹œëœ ê²½ë¡œ ì‚¬ìš©: {news.n_image_url}")
            continue

        # 2. íŒŒì¼ ì´ë¦„ ìƒì„± (URL í•´ì‹œ ê¸°ë°˜)
        try:
            url_hash = hashlib.sha256(external_url.encode('utf-8')).hexdigest()
            # íŒŒì¼ í™•ì¥ì ìœ ì§€, ì—†ìœ¼ë©´ .jpg ì‚¬ìš©
            file_extension = Path(external_url.split("?")[0]).suffix or '.jpg'
            save_name = f"{url_hash}{file_extension}"
            save_path = NEWS_IMG_DIR / save_name
            local_db_path = f"img/news/{save_name}"
        except Exception as e:
            print(f"  -> íŒŒì¼ ì´ë¦„ ìƒì„± ì‹¤íŒ¨: {e}")
            continue

        # 3. ë””ìŠ¤í¬ì— íŒŒì¼ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        if save_path.exists():
            print(f"  -> ê¸°ì¡´ íŒŒì¼ ì¬ì‚¬ìš©: {local_db_path}")
            news.n_image_url = local_db_path
            news.save(update_fields=["n_image_url"])
            processed_urls[external_url] = local_db_path  # ìºì‹œì— ì¶”ê°€
            continue

        # 4. ì´ë¯¸ì§€ê°€ ì—†ë‹¤ë©´ ë‹¤ìš´ë¡œë“œ
        print(f"  -> ìƒˆ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ: {save_name}")
        local_path = download_news_image(
            external_url,
            save_name,
            max_size_kb=max_size_kb
        )

        if local_path:
            news.n_image_url = local_path
            news.save(update_fields=["n_image_url"])
            processed_urls[external_url] = local_path  # ìºì‹œì— ì¶”ê°€

        time.sleep(0.5)

    print("âœ… ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ")
